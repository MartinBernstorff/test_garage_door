<!doctype html><html lang=en>
<head>
<meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge">
<meta name=viewport content="width=device-width,initial-scale=1">
<meta name=description content="Feature Engineering
Q. What does one-hot encoding mean?
A. Transforming a categorical variable into a vector with as many dimensions as the number of categories
Q. Which data-management process is this an example of?

A. One-hot encoding
Q. If you have a feature with categories [red, yellow, green], …">
<meta name=apple-mobile-web-app-capable content="yes">
<meta name=mobile-web-app-capable content="yes">
<meta name=apple-mobile-web-app-status-bar-style content="default">
<link rel=manifest href=https://martbern.github.io/test_garage_door/manifest.json><meta property="og:title" content>
<meta property="og:description" content="Feature Engineering Q. What does one-hot encoding mean? A. Transforming a categorical variable into a vector with as many dimensions as the number of categories
Q. Which data-management process is this an example of? A. One-hot encoding
Q. If you have a feature with categories [red, yellow, green], which process might you use to encode those in the dataset? A. One-hot encoding Q. Why might we use binning for ML? A.">
<meta property="og:type" content="article">
<meta property="og:url" content="https://martbern.github.io/test_garage_door/feature-engineering.html"><meta property="article:section" content>
<meta name=twitter:card content="summary">
<meta name=twitter:title content>
<meta name=twitter:description content="Feature Engineering Q. What does one-hot encoding mean? A. Transforming a categorical variable into a vector with as many dimensions as the number of categories
Q. Which data-management process is this an example of? A. One-hot encoding
Q. If you have a feature with categories [red, yellow, green], which process might you use to encode those in the dataset? A. One-hot encoding Q. Why might we use binning for ML? A.">
<title>Feature Engineering - My New Hugo Site</title>
<link rel=stylesheet href=https://martbern.github.io/test_garage_door/css/main.min.33389b11303e8fae9369acbd75cced8bf796d575b1120d23baabdf9ab8d6b6d4.css>
<script src=https://ajax.googleapis.com/ajax/libs/jquery/3.6.0/jquery.min.js></script>
<script src=https://martbern.github.io/test_garage_door/js/main.min.f64522e8eae3046c0982972c612443849ea16544a60448073f6eb7f75f8c622d.js integrity="sha256-9kUi6OrjBGwJgpcsYSRDhJ6hZUSmBEgHP26391+MYi0="></script>
</head>
<body>
<style>search-menu{display:block}#search{height:100%;width:0;position:fixed;background:var(--background-search);z-index:1;top:0;left:0;border-right:1px solid var(--separator-color);overflow-x:hidden;overflow-y:auto;opacity:0;-ms-overflow-style:none;scrollbar-width:none}#search::-webkit-scrollbar{display:none}#search-header{padding:12px;position:fixed;padding-left:12px;padding-right:12px;background:var(--background-search);width:250px;opacity:1;height:50px;z-index:2;border-bottom:1px solid var(--separator-color)}#search .input-container{position:relative}#search-input{width:100%;height:24px;border:1px solid var(--separator-color);border-radius:4px;padding-left:16px;background-color:#fff;display:inline-block}#search-input:focus{border:1px solid var(--search-field-focused-color)}#search-header .input-container .search-icon{position:absolute;top:6px;left:8px;fill:#a9a9a9}#search-results img{width:122px;height:76px;border:1px solid var(--separator-color);object-fit:cover}#search-results{margin-top:50px;overflow:auto;height:100%}#search-results a{width:100%;padding-left:25px;padding-right:25px;padding-top:12px;padding-bottom:12px;display:inline-block;color:var(--text-base-color);border-bottom:1px solid var(--separator-color);border-left:6px solid var(--background-search)}#search-results a:first-child:hover,a:first-child:focus,.selected{outline:0;background-color:var(--note-table-cell-selected-color);border-left:6px solid var(--note-table-cell-ribbon-color)!important}#search-results li{text-indent:0}#search-results li:before,#search-results h1:before,#search-results h2:before,#search-results h3:before,#search-results h4:before,#search-results h5:before,#search-results h6:before{content:"";visibility:hidden;display:none}</style>
<search-menu id=search data-turbolinks-permanent>
<header id=search-header>
<div class=input-container><svg aria-hidden="true" class="search-icon" width="12" height="12" viewBox="0 0 18 18"><path d="M18 16.5l-5.14-5.18h-.35a7 7 0 10-1.19 1.19v.35L16.5 18l1.5-1.5zM12 7A5 5 0 112 7a5 5 0 0110 0z"/></svg>
<input type=search autocomplete=off id=search-input onkeyup=performSearch() tabindex=0 placeholder="Search note">
</div>
</header>
<ul id=search-results></ul>
</search-menu>
<script></script>
<style>#toolbar{position:fixed;top:0;right:0;width:60px;height:100%;display:flex;flex-direction:column;justify-content:flex-start;align-items:center;transition:1s;opacity:.5;padding:18px 0}#toolbar:hover{opacity:1}#close-nav-icon{display:none}</style>
<aside id=toolbar>
<span style=cursor:pointer id=open-nav-icon onclick=handleNavVisibility()><svg width="18" height="18" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><circle fill="none" stroke="var(--text-base-color)" stroke-width="1.1" cx="9" cy="9" r="7"/><path fill="none" stroke="var(--text-base-color)" stroke-width="1.1" d="M14 14l4 4-4-4z"/></svg>
</span>
</aside>
<main id=main>
<div id=note-wrapper class=note-wrapper>
<h1 id=feature-engineering>Feature Engineering</h1>
<p>Q. What does one-hot encoding mean?
A. Transforming a categorical variable into a vector with as many dimensions as the number of categories</p>
<p>Q. Which data-management process is this an example of?
<img src=BearImages/4C1AB092-8006-405C-B0EB-A51265DFD81D-5010-00000F1E2347A0A5/359ADC12-613F-46B2-97E7-4DFDCC065901.png alt>
A. One-hot encoding</p>
<p>Q. If you have a feature with categories [red, yellow, green], which process might you use to encode those in the dataset?
A. One-hot encoding
<img src=BearImages/18CF63B8-88D8-43D7-BABA-47D2FD39C488-5010-0000125945364830/359ADC12-613F-46B2-97E7-4DFDCC065901.png alt></p>
<p>Q. Why might we use binning for ML?
A. For a continuous variable, it “hints” to the algorithm that the rough category is what matters</p>
<p>Q. Why might we want to normalise data for machine learning?
A. 1) Faster speed of learning and 2) Decreased risk of numerical overflow</p>
<p>Q. Why might normalising data lead to faster gradient descent?
A. Rather than features with large numbers dominating the gradients, each feature counts equally</p>
<p>Q. How does normalisation differ from standardisation?
A. <a href=normalisation.html>Normalisation</a> is standardising to a normal distribution, although the terminology is not strict in practice.</p>
<p>In which cases are [0,1] standardisation preferable to normalisation?</p>
<ol>
<li>Unsupervised learning, 2) If the value is normally distributed and 3) With outliers</li>
</ol>
<p>When might we prefer normalisation to [0,1] standardisation?</p>
<ol>
<li>Supervised learning, 2) Values are not normally distributed and 3) No outliers</li>
</ol>
<h2 id=backlinks>Backlinks</h2>
<ul>
<li><a href=machine-learning.html>§Machine Learning</a>
<ul>
<li><a href=feature-engineering.html>Feature Engineering</a></li>
</ul>
</li>
</ul>