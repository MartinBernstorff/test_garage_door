# BERT
En sub-model af [[The Transformer Architecture]].

Q. Which tasks is BERT trained on during pre-training?
A. Masked language modelling (masking N words in a sentence)

> Out-of-the-box BERT is surprisingly brittle to (Devlin et al., 2018a) have become increasingly named entity replacements: e.g. replacing names important in NLP. They are optimised to either in the coreference task changes 85% of predictions predict the next word in a sequence or some (Balasubramanian et al., 2020).

Q. How does the encoding of a token in BERT differ from a word embedding? Is this correct? Rewrite after talking to #Collaborators/Kenneth .
A. The BERT-encoding includes information from the context that the word typically occurs in. 



<!-- #anki/deck/ML -->

<!-- {BearID:100DCED3-0254-48AE-B543-E1C35CD21C03-93658-000001B846937CF9} -->
