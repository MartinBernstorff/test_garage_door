# Operationalising meta-ethics
Rules are ways of verifying whether an action was good.

[Notion – The all-in-one workspace for your notes, tasks, wikis, and databases.](https://www.notion.so/How-to-Operationalize-Metaethics-2919bbd6a882487fb3083029feaceffa)

*Rule 1: Actions are good if they make us happier*
Happier now, in the future or in total? If you want both now and in the future, and those struggle, an external agent is likely to decrease the balance between those (*taking sides*).

Furthermore, sometimes we don't want to be happy. We don't want to be force-fed drugs to feel bliss when our friend just died, and it removes agency.

*Rule 2: Actions are good if they help us reach our goals*
![](BearImages/173CE9C4-02C8-49EA-9155-C0F4676B25BB-40235-000047B49B3195F2/E4ED4AD1-F342-486B-9F44-90D77137A4E9.png)
An *inceptive* rule is one under which it's easy to justify interventions which aren't actually beneficial. A rule for genies and mad scientists.

We often have a ton of goals. If a separate agent wants to, they might optimise for a goal that isn't terribly important to us.

Furthermore, some things (our values, like being free, or "being dutiful") can't be specified as goal states, but are nonetheless important. Eg. writing an essay can be both free and forced.

Lastly, some harms aren't goal-states either, like not feeling like agency.

*Rule 3: Actions are good if they help more people satisfy their revealed preferences*
![](BearImages/7F248C39-3B65-4A76-95B5-94B24B84EB18-40235-000047B6C318043C/494E7F18-B360-443A-8E03-BAE8545FC993.png)

*Rule 4: Actions are good if they advance our well-being or flourishing*
![](BearImages/6D50F9E6-EC79-4A25-8838-E58985C67E3B-40235-000047CC17CED5B6/70F517FB-4460-4C56-8364-40E4CC82D343.png)
![](BearImages/BCAFA702-9CF7-4924-99B7-F1106AC722AE-40235-000047ECE79EF853/EBF602D8-20EC-462A-B227-788D4D373AD3.png)
Is vulnerable by not being possible to check, and by being immensely important to nudge if possible (eg. bad actors would be highly incentivised to change this universal algorithm).

Also amounts to choosing "all of the above" for the rules, inheriting my of the vulnerabilites.

*Rule 4b: Actions are good if they advance our knowledge* 

*Rule 6: Actions are good if they help us live by our values*
![](BearImages/0C21DB62-5FFD-4001-8142-B793140DC240-40235-00004826881FB496/DDBF8B22-5DC1-4BC5-A466-62D1173A533C.png)

One problem, it doesn't facilitate figuring out which values are important.

*Rule 6b: Actions are good if they help us live by our present and future values*

> Because a person’s values may diverge from others’ values and still be right, and because they can be intuitive, inexplicable, and result from experimentation and reflection which only that one person has done, a person must be considered the authority as to their own values.

> To sum up, values are debatable, intrapersonally convergent, and interpersonally divergent. These traits, together, mean that a person is the authority about which values work for them.

For that reason, we might want to support people to either a) Live by their values og b) Experiment with new values that might better fit them.

![](BearImages/C43A525B-B187-4BA1-BD67-0A53E2C701C8-1082-00000227AB5FA05A/0122A91F-AFD2-4F3B-BA0B-1A795C521357.png)

However, we might sometimes live as a social performance (to /seem/ happy) rather than on our own terms (to /be/ happy). To avoid that, we might ask about it.

![](BearImages/948DABED-E60A-4EE7-9BF6-012198D41CE8-1082-000002ADA0C35A23/6A2118C3-C787-43EB-AB0F-9EF530FD3C5C.png)

<!-- {BearID:9A4F7A21-8075-4865-A259-761AC5CCFC00-40235-00004663CD9C832F} -->
