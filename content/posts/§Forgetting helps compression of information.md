# §Forgetting helps compression of information
In the sense that we forget the unimportant, but retain the important – the important we encounter again and again as prediction errors and retain, whereas the unimportant is forgotten not to be re-encountered.

What is important? The features that matter for predicting reality and counter-factual realities. You might consider that the "important" features of the set /X/ are the features required for predicting the outcome /Y/, and *only* those features. 

Maximum compression and avoiding over-fitting is what forgetting is all about.

What is unimportant? Noise, or features that are too weak to store and integrate into a coherent framework.

The idea was sprouted by [Quanta Magazine](https://www.quantamagazine.org/new-theory-cracks-open-the-black-box-of-deep-learning-20170921/) and:
![](BearImages/4AEDF796-0BB3-426C-9EFC-0D1772122C27-12104-000031C474C56DE5/DeepLearning_5001.jpg)

<!-- #p1 -->

## Backlinks
* [[§Forgetting]]
	* [[§Forgetting helps compression of information]]
* [[Do I want to suspend Anki cards when they rise above a certain interval]]
	* E.g. because the purpose of [[§Forgetting]] is that [[§Forgetting helps compression of information]].

<!-- {BearID:7D8D0028-50A7-4BA0-A878-35CEAB48CEC5-12104-000031B6375D8FAD} -->
